# 음식점이미지크롤링

# icrawler 사용 시

## install

```python
pip install icrawler  # 또는
conda install -c hellock icrawler
```

## Requirements

Python 2.7+ or 3.5+ (recommended).

## Example

```python
from icrawler.builtin import GoogleImageCrawler

google_crawler = GoogleImageCrawler(storage={'root_dir': 'your_image_dir'})
google_crawler.crawl(keyword='검색어값', max_num=100)
```

your_image_dir : 이미지를 저장할 위치 설정

keyword : 구글에 검색할 값

max_num : 저장할 사진 최대 개수

## 공식 문서

- icrawler

    [](https://pypi.org/project/icrawler/)

- 확장 시

    [](https://icrawler.readthedocs.io/en/latest/extend.html)

    Downloader 확인해보자!

# beautifulsoup4 사용시

## install

```python
pip install beautifulsoup4
```

## example

```python
from bs4 import BeautifulSoup as bs
from urllib.request import urlopen
from urllib.parse import quote_plus
 
baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='
plusUrl = input('검색어 입력: ') 
crawl_num = int(input('크롤링할 갯수 입력(최대 50개): '))
 
url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환
html = urlopen(url)
soup = bs(html, "html.parser")
img = soup.find_all(class_='_img')
 
n = 1
for i in img:
    print(n)
    imgUrl = i['data-source']
    with urlopen(imgUrl) as f:
        with open('./images/img' + str(n)+'.jpg','wb') as h: # w - write b - binary
            img = f.read()
            h.write(img)
    n += 1
    if n > crawl_num:
        break
    
    
print('Image Crawling is done.')

```

images 폴더 밑으로 img1.jpg, img2.jpg, img3.jpg,...등 이름으로 이미지 저장됨

네이버의 경우 최대 50개의 이미지만 크롤링 가능한 것 같음

## 참고한 사이트

[[python] 웹에서 이미지 수집하기, 이미지 크롤링 (beautifulsoup 활용)](https://bskyvision.com/721)

## 이외의 참고 사이트

[Google image crawler / Crawling / Scraping / python](https://ecsimsw.tistory.com/entry/Google-image-crawler-Crawling-Scraping-python)

[파이썬으로 네이버 이미지 크롤링하기 (Image Crawling)](https://ultrakid.tistory.com/13)